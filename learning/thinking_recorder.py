"""
AIæ€è€ƒè¿‡ç¨‹è®°å½•å·¥å…· - Window 9æ ¸å¿ƒç»„ä»¶
è®°å½•æ‰€æœ‰AIæ€è€ƒè¿‡ç¨‹ï¼Œç”Ÿæˆå¯å¤ç”¨çš„å­¦ä¹ èµ„æ–™
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import pickle
from uuid import uuid4


class ThinkingRecorder:
    """è®°å½•æ‰€æœ‰AIæ€è€ƒè¿‡ç¨‹ï¼Œç”Ÿæˆå­¦ä¹ èµ„æ–™"""
    
    def __init__(self, base_path: str = "learning_data"):
        """
        åˆå§‹åŒ–æ€è€ƒè®°å½•å™¨
        
        Args:
            base_path: å­¦ä¹ æ•°æ®å­˜å‚¨åŸºç¡€è·¯å¾„
        """
        self.base_path = Path(base_path)
        self._init_directories()
        
    def _init_directories(self):
        """åˆå§‹åŒ–å­˜å‚¨ç›®å½•ç»“æ„"""
        directories = [
            self.base_path / "thinking" / "json",
            self.base_path / "thinking" / "markdown", 
            self.base_path / "thinking" / "training",
            self.base_path / "decisions",
            self.base_path / "signals",
            self.base_path / "patterns",
            self.base_path / "weights"
        ]
        
        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)
            
    def record_thinking_process(self, decision_id: str, thinking_chain: List[Dict]) -> Dict:
        """
        è®°å½•å®Œæ•´çš„10æ­¥æ€è€ƒé“¾
        
        Args:
            decision_id: å†³ç­–ID
            thinking_chain: æ€è€ƒé“¾åˆ—è¡¨
            
        Returns:
            è®°å½•çš„å®Œæ•´ä¿¡æ¯
        """
        record = {
            "decision_id": decision_id,
            "timestamp": datetime.now().isoformat(),
            "thinking_steps": self._format_thinking_steps(thinking_chain),
            "final_decision": thinking_chain[-1] if thinking_chain else None,
            "confidence": self._calculate_confidence(thinking_chain),
            "metadata": {
                "ai_model": "Claude Opus 4.1",
                "version": "1.0",
                "market_condition": self._analyze_market_condition(thinking_chain)
            }
        }
        
        # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
        self.save_as_learning_material(record)
        
        return record
        
    def _format_thinking_steps(self, thinking_chain: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–æ€è€ƒæ­¥éª¤"""
        formatted_steps = []
        
        step_names = [
            "æ•°æ®æ”¶é›†",
            "å¼‚å¸¸è¯†åˆ«", 
            "æ¨¡å¼åŒ¹é…",
            "é£é™©è¯„ä¼°",
            "æœºä¼šè¯†åˆ«",
            "ç­–ç•¥é€‰æ‹©",
            "ä»“ä½è®¡ç®—",
            "æ—¶æœºåˆ¤æ–­",
            "æ‰§è¡Œè®¡åˆ’",
            "å†³ç­–è¾“å‡º"
        ]
        
        for i, (step_name, content) in enumerate(zip(step_names, thinking_chain), 1):
            formatted_step = {
                "step": i,
                "name": step_name,
                "content": content,
                "timestamp": datetime.now().isoformat(),
                "data_sources": self._extract_data_sources(content),
                "patterns_found": self._extract_patterns(content) if i == 3 else [],
                "risks_identified": self._extract_risks(content) if i == 4 else [],
                "opportunities": self._extract_opportunities(content) if i == 5 else []
            }
            formatted_steps.append(formatted_step)
            
        return formatted_steps
        
    def _extract_data_sources(self, content: Dict) -> List[str]:
        """æå–æ•°æ®æº"""
        sources = []
        if isinstance(content, dict):
            if "sources" in content:
                sources = content["sources"]
            elif "data_from" in content:
                sources = content["data_from"]
            else:
                # ä»å†…å®¹ä¸­æ¨æ–­æ•°æ®æº
                text = str(content)
                if "Window2" in text or "exchange" in text.lower():
                    sources.append("Window2_Exchange")
                if "Window3" in text or "collector" in text.lower():
                    sources.append("Window3_Collector")
                if "Window4" in text or "analysis" in text.lower():
                    sources.append("Window4_Analysis")
                    
        return sources
        
    def _extract_patterns(self, content: Dict) -> List[Dict]:
        """æå–å‘ç°çš„æ¨¡å¼"""
        patterns = []
        if isinstance(content, dict) and "patterns" in content:
            patterns = content["patterns"]
        return patterns
        
    def _extract_risks(self, content: Dict) -> List[Dict]:
        """æå–è¯†åˆ«çš„é£é™©"""
        risks = []
        if isinstance(content, dict) and "risks" in content:
            risks = content["risks"]
        return risks
        
    def _extract_opportunities(self, content: Dict) -> List[Dict]:
        """æå–æœºä¼š"""
        opportunities = []
        if isinstance(content, dict) and "opportunities" in content:
            opportunities = content["opportunities"]
        return opportunities
        
    def _calculate_confidence(self, thinking_chain: List[Dict]) -> float:
        """è®¡ç®—å†³ç­–ä¿¡å¿ƒåº¦"""
        if not thinking_chain:
            return 0.0
            
        # åŸºäºæ€è€ƒé“¾çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§è®¡ç®—ä¿¡å¿ƒåº¦
        base_confidence = 0.5
        
        # å®Œæ•´æ€§åŠ åˆ†
        if len(thinking_chain) >= 10:
            base_confidence += 0.2
            
        # æ£€æŸ¥æ˜¯å¦æœ‰é£é™©è¯„ä¼°
        has_risk_assessment = any("risk" in str(step).lower() for step in thinking_chain)
        if has_risk_assessment:
            base_confidence += 0.15
            
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®æ”¯æ’‘
        has_data_support = any("data" in str(step).lower() for step in thinking_chain)
        if has_data_support:
            base_confidence += 0.15
            
        return min(base_confidence, 1.0)
        
    def _analyze_market_condition(self, thinking_chain: List[Dict]) -> str:
        """åˆ†æå¸‚åœºçŠ¶å†µ"""
        # ä»æ€è€ƒé“¾ä¸­æå–å¸‚åœºçŠ¶å†µ
        conditions = []
        
        for step in thinking_chain:
            step_str = str(step).lower()
            if "volatile" in step_str or "æ³¢åŠ¨" in step_str:
                conditions.append("volatile")
            if "trend" in step_str or "è¶‹åŠ¿" in step_str:
                conditions.append("trending")
            if "sideways" in step_str or "æ¨ªç›˜" in step_str:
                conditions.append("sideways")
                
        if "volatile" in conditions:
            return "high_volatility"
        elif "trending" in conditions:
            return "trending"
        elif "sideways" in conditions:
            return "sideways"
        else:
            return "normal"
            
    def save_as_learning_material(self, record: Dict):
        """
        ç”Ÿæˆå›ºå®šæ ¼å¼çš„å­¦ä¹ èµ„æ–™
        
        Args:
            record: è®°å½•æ•°æ®
        """
        decision_id = record['decision_id']
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆç¨‹åºè¯»å–ï¼‰
        json_path = self.base_path / "thinking" / "json" / f"{decision_id}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜ä¸ºMarkdownæ ¼å¼ï¼ˆäººç±»å¯è¯»ï¼‰
        md_path = self.base_path / "thinking" / "markdown" / f"{decision_id}.md"
        self._save_as_markdown(record, md_path)
        
        # ä¿å­˜ä¸ºè®­ç»ƒæ•°æ®æ ¼å¼ï¼ˆä¸ºæœªæ¥æ¨¡å‹å‡†å¤‡ï¼‰
        training_path = self.base_path / "thinking" / "training" / f"{decision_id}.pkl"
        with open(training_path, 'wb') as f:
            pickle.dump(record, f)
            
    def _save_as_markdown(self, record: Dict, path: Path):
        """ä¿å­˜ä¸ºMarkdownæ ¼å¼"""
        md_content = f"""# AIæ€è€ƒè¿‡ç¨‹è®°å½•

## å†³ç­–ID: {record['decision_id']}
**æ—¶é—´**: {record['timestamp']}  
**ä¿¡å¿ƒåº¦**: {record['confidence']:.2%}  
**å¸‚åœºçŠ¶å†µ**: {record['metadata']['market_condition']}  

## æ€è€ƒæ­¥éª¤

"""
        
        for step in record['thinking_steps']:
            md_content += f"""### æ­¥éª¤ {step['step']}: {step['name']}

**å†…å®¹**: 
```json
{json.dumps(step['content'], ensure_ascii=False, indent=2)}
```

"""
            if step.get('data_sources'):
                md_content += f"**æ•°æ®æº**: {', '.join(step['data_sources'])}\n\n"
                
            if step.get('patterns_found'):
                md_content += f"**å‘ç°çš„æ¨¡å¼**: \n"
                for pattern in step['patterns_found']:
                    md_content += f"- {pattern}\n"
                md_content += "\n"
                
            if step.get('risks_identified'):
                md_content += f"**è¯†åˆ«çš„é£é™©**: \n"
                for risk in step['risks_identified']:
                    md_content += f"- {risk}\n"
                md_content += "\n"
                
            if step.get('opportunities'):
                md_content += f"**æœºä¼š**: \n"
                for opp in step['opportunities']:
                    md_content += f"- {opp}\n"
                md_content += "\n"
                
        md_content += f"""## æœ€ç»ˆå†³ç­–

```json
{json.dumps(record['final_decision'], ensure_ascii=False, indent=2)}
```

---
*ç”±Window 9å­¦ä¹ ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        with open(path, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
    def get_recent_thoughts(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æ€è€ƒè®°å½•"""
        json_dir = self.base_path / "thinking" / "json"
        
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = sorted(json_dir.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
        
        records = []
        for json_file in json_files[:limit]:
            with open(json_file, 'r', encoding='utf-8') as f:
                records.append(json.load(f))
                
        return records
        
    def analyze_thinking_patterns(self) -> Dict:
        """åˆ†ææ€è€ƒæ¨¡å¼"""
        records = self.get_recent_thoughts(100)
        
        analysis = {
            "total_records": len(records),
            "average_confidence": 0,
            "market_conditions": {},
            "common_patterns": [],
            "success_indicators": []
        }
        
        if not records:
            return analysis
            
        # è®¡ç®—å¹³å‡ä¿¡å¿ƒåº¦
        total_confidence = sum(r.get('confidence', 0) for r in records)
        analysis['average_confidence'] = total_confidence / len(records)
        
        # ç»Ÿè®¡å¸‚åœºçŠ¶å†µ
        for record in records:
            condition = record.get('metadata', {}).get('market_condition', 'unknown')
            analysis['market_conditions'][condition] = analysis['market_conditions'].get(condition, 0) + 1
            
        return analysis


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    recorder = ThinkingRecorder()
    
    # æ¨¡æ‹Ÿæ€è€ƒé“¾
    test_thinking_chain = [
        {"step": "collect", "data": "å¸‚åœºæ•°æ®æ”¶é›†", "sources": ["Window2", "Window3"]},
        {"step": "identify", "anomalies": ["ä»·æ ¼å¼‚å¸¸", "æˆäº¤é‡æ¿€å¢"]},
        {"step": "match", "patterns": ["çªç ´å½¢æ€", "é‡ä»·èƒŒç¦»"]},
        {"step": "assess", "risks": ["é«˜æ³¢åŠ¨é£é™©", "æµåŠ¨æ€§é£é™©"]},
        {"step": "find", "opportunities": ["åšå¤šæœºä¼š", "å¥—åˆ©æœºä¼š"]},
        {"step": "select", "strategy": "è¶‹åŠ¿è·Ÿè¸ª"},
        {"step": "calculate", "position": "10% ä»“ä½"},
        {"step": "timing", "entry": "ç«‹å³è¿›åœº"},
        {"step": "plan", "execution": "é™ä»·å•å…¥åœº"},
        {"step": "decide", "action": "BUY", "symbol": "BTC/USDT", "confidence": 0.85}
    ]
    
    # è®°å½•æ€è€ƒè¿‡ç¨‹
    decision_id = str(uuid4())
    result = recorder.record_thinking_process(decision_id, test_thinking_chain)
    
    print(f"âœ… æ€è€ƒè¿‡ç¨‹å·²è®°å½•")
    print(f"å†³ç­–ID: {result['decision_id']}")
    print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.2%}")
    print(f"å¸‚åœºçŠ¶å†µ: {result['metadata']['market_condition']}")
    
    # åˆ†ææ€è€ƒæ¨¡å¼
    analysis = recorder.analyze_thinking_patterns()
    print(f"\nğŸ“Š æ€è€ƒæ¨¡å¼åˆ†æ:")
    print(f"æ€»è®°å½•æ•°: {analysis['total_records']}")
    print(f"å¹³å‡ä¿¡å¿ƒåº¦: {analysis['average_confidence']:.2%}")